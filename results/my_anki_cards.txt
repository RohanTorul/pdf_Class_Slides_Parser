Exploitation vs. Exploration (Page 2)	One of the main challenges in RL is the trade-off
Exploitation vs. Exploration (Page 2)	between exploitation and exploration
Exploitation vs. Exploration (Page 2)	To obtain a lot of reward, agents must prefer
Exploitation vs. Exploration (Page 2)	actions that it knows produce good results
Exploitation vs. Exploration (Page 2)	In order to learn which actions produce good
Exploitation vs. Exploration (Page 2)	rewards, it must try them out first
Exploitation vs. Exploration (Page 2)	The agent must exploit knowledge it has, but
Exploitation vs. Exploration (Page 2)	also explore in order to gain more knowledge
Exploitation vs. Exploration (Page 2)	Also one of the main challenges of real life
Learning (Page 3)	Learning is the process of gaining new (or
Learning (Page 3)	modifying existing) knowledge
Learning (Page 3)	We have learned something if we have
Learning (Page 3)	information that we didn‚Äôt have before
Learning (Page 3)	Even if we are given same results, we
Learning (Page 3)	learn to trust them with more certainty
Learning (Page 3)	(our estimate has less variance)
Exploitation (Page 4)	Exploit our current knowledge
Exploitation (Page 4)	Choose high-valued actions for which we
Exploitation (Page 4)	already know the value
Exploitation (Page 4)	Low risk, Low Reward (possibly)
Exploitation (Page 4)	We know what we will get
Exploitation (Page 4)	Always choosing actions we are familiar
Exploitation (Page 4)	with gives us little new information
Exploration (Page 5)	Try actions we haven‚Äôt before
Exploration (Page 5)	Helps us learn their values
Exploration (Page 5)	Possible that new actions have higher
Exploration (Page 5)	rewards than previously selected
Exploration (Page 5)	Once we have explored sufficiently, then
Exploration (Page 5)	we can exploit the best actions and know
Exploration (Page 5)	that they are the best
N-Armed Bandit Problem (Page 6)	Repeatedly make a choice
N-Armed Bandit Problem (Page 6)	among n different actions
N-Armed Bandit Problem (Page 6)	After each action you receive a
N-Armed Bandit Problem (Page 6)	reward from a stationary
N-Armed Bandit Problem (Page 6)	probability distribution
N-Armed Bandit Problem (Page 6)	depending on the action
N-Armed Bandit Problem (Page 6)	Objective is to maximize your
N-Armed Bandit Problem (Page 6)	expected total reward over a
N-Armed Bandit Problem (Page 6)	number of action selections
N-Armed Bandit Problem (Page 7)	Also called Multi-Armed Bandit
N-Armed Bandit Problem (Page 7)	Name is an analogy to slot machines:
N-Armed Bandit Problem (Page 7)	‚ÄúOne-armed bandit‚Äù
N-Armed Bandit Problem (Page 7)	You have a limited amount of money, and
N-Armed Bandit Problem (Page 7)	you try to win as much as possible
N-Armed Bandit Problem (Page 7)	How do we select which levers to pull?
Exploitation vs. Exploration (Page 9)	If we maintain an estimate of action values, at
Exploitation vs. Exploration (Page 9)	any time there is one greatest
Exploitation vs. Exploration (Page 9)	The ‚ÄòGreedy Action‚Äô
Exploitation vs. Exploration (Page 9)	Exploitation: Choosing the Greedy Action
Exploitation vs. Exploration (Page 9)	Maximizes single action returns
Exploitation vs. Exploration (Page 9)	Exploration: Choosing a non-greedy action to
Exploitation vs. Exploration (Page 9)	improve your action estimates
Exploitation vs. Exploration (Page 9)	Required for future reward maximization
Exploitation vs. Exploration (Page 9)	How to balance exploitation vs. exploration?
Two Important EvE Concepts (Page 10)	How to store and update value estimates
Two Important EvE Concepts (Page 10)	1.
Two Important EvE Concepts (Page 10)	as we learn over time from new info
Two Important EvE Concepts (Page 10)	How to choose which action to do next
Two Important EvE Concepts (Page 10)	2.
Two Important EvE Concepts (Page 10)	based on our current value estimate
Action-Value Methods (Page 11)	How to store the current value estimate?
Action-Value Methods (Page 11)	Q (a) = Estimate of Q*(a) after time step t
Action-Value Methods (Page 11)	t
Action-Value Methods (Page 11)	Q*(a) = Actual Value of action a
Action-Value Methods (Page 11)	‚ÄúActual Value‚Äù = Mean Reward
Action-Value Methods (Page 11)	As time t goes to infinity
Q-Value Implementation (Page 12)	Example: N-Armed Bandit Problem
Q-Value Implementation (Page 12)	Q(a) = Estimate of bandit a reward
Q-Value Implementation (Page 12)	Choosing a bandit = one action
Q-Value Implementation (Page 12)	Q can be an array of size n for n bandits
Q-Value Implementation (Page 12)	Q = [0, 0, 0, 0] n=4
Q-Value Implementation (Page 12)	0
Q-Value Implementation (Page 12)	After some number of time steps t
Q-Value Implementation (Page 12)	Q = [4, 7, 22, 10]
Q-Value Implementation (Page 12)	t
Value Estimation Methods (Page 20)	Two main variants of estimating value
Value Estimation Methods (Page 20)	Sample Average Estimation
Value Estimation Methods (Page 20)	1.
Value Estimation Methods (Page 20)	Incremental Update Estimation
Value Estimation Methods (Page 20)	2.
Q (a) Sample Average Estimation (Page 21)	t
Q (a) Sample Average Estimation (Page 21)	Natural way of calculating Q (a) is to average the
Q (a) Sample Average Estimation (Page 21)	t
Q (a) Sample Average Estimation (Page 21)	rewards received so far after a number of plays
Q (a) Sample Average Estimation (Page 21)	If at play t, action a has been chosen k times,
Q (a) Sample Average Estimation (Page 21)	a
Q (a) Sample Average Estimation (Page 21)	yielding rewards r , r , ‚Ä¶, r then:
Q (a) Sample Average Estimation (Page 21)	1 2 k
Q (a) Sample Average Estimation (Page 21)	a
Q (a) Sample Average Estimation (Page 21)	Q (a) = (r +r +‚Ä¶+r ) / k
Q (a) Sample Average Estimation (Page 21)	t 1 2 k a
Q (a) Sample Average Estimation (Page 21)	a
Q (a) Sample Average Estimation (Page 21)	If k = 0, define Q as some default, Q (a) = 0
Q (a) Sample Average Estimation (Page 21)	a t t
Q (a) Sample Average Estimation (Page 21)	As k gets large, Q (a) converges to Q*(a)
Q (a) Sample Average Estimation (Page 21)	a t
Q (a) Sample Average Estimation (Page 21)	Average of samples = ‚Äúsample average‚Äù method
Value Estimation Methods (Page 31)	Two main variants of estimating value
Value Estimation Methods (Page 31)	Sample Average Estimation
Value Estimation Methods (Page 31)	1.
Value Estimation Methods (Page 31)	Incremental Update Estimation
Value Estimation Methods (Page 31)	2.
Incremental Action-Value Est. (Page 32)	Recall: Q (a) = (r +r +‚Ä¶+r ) / k
Incremental Action-Value Est. (Page 32)	t 1 2 k a
Incremental Action-Value Est. (Page 32)	a
Incremental Action-Value Est. (Page 32)	Need to store all of the rewards
Incremental Action-Value Est. (Page 32)	Problem: Memory and computational requirements
Incremental Action-Value Est. (Page 32)	grow over time
Incremental Action-Value Est. (Page 32)	Let‚Äôs derive an incremental formula so that
Incremental Action-Value Est. (Page 32)	memory is no longer an issue
Incremental Action-Value Est. (Page 32)	Have: NewAvg = F(history of all samples)
Incremental Action-Value Est. (Page 32)	Wanted: NewAvg = F(OldAverage, NewSample)
Incremental Average (Page 33)	Q = (r + r + ‚Ä¶ + r ) / k
Incremental Average (Page 33)	k 1 2 k
Incremental Average (Page 33)	1
Incremental Average (Page 33)	=
Incremental Average (Page 33)	(œÉùëò
Incremental Average (Page 33)	ùëü )
Incremental Average (Page 33)	ùëñ=1 ùëñ
Incremental Average (Page 33)	k
Incremental Average (Page 33)	1
Incremental Average (Page 33)	Q =
Incremental Average (Page 33)	(œÉùëò+1
Incremental Average (Page 33)	ùëü )
Incremental Average (Page 33)	k+1 ùëñ=1 ùëñ
Incremental Average (Page 33)	k+1
Incremental Average (Page 33)	1
Incremental Average (Page 33)	=
Incremental Average (Page 33)	(œÉùëò
Incremental Average (Page 33)	ùëü + r )
Incremental Average (Page 33)	ùëñ=1 ùëñ k+1
Incremental Average (Page 33)	k+1
Incremental Average (Page 33)	1
Incremental Average (Page 33)	= (kQ + r )
Incremental Average (Page 33)	k k+1
Incremental Average (Page 33)	k+1
Incremental Average (Page 33)	1
Incremental Average (Page 33)	= (r + kQ + Q - Q )
Incremental Average (Page 33)	k+1 k k k
Incremental Average (Page 33)	k+1
Incremental Average (Page 33)	1
Incremental Average (Page 33)	= (r + (k+1)Q - Q )
Incremental Average (Page 33)	k+1 k k
Incremental Average (Page 33)	k+1
Incremental Average (Page 33)	1
Incremental Average (Page 33)	= Q + (r - Q )
Incremental Average (Page 33)	k k+1 k
Incremental Average (Page 33)	k+1
Incremental Implementation (Page 34)	Q = average of first k rewards
Incremental Implementation (Page 34)	k
Incremental Implementation (Page 34)	1
Incremental Implementation (Page 34)	ùëò+1
Incremental Implementation (Page 34)	Q = œÉ ùëü
Incremental Implementation (Page 34)	k+1 ùëñ=1 ùëñ
Incremental Implementation (Page 34)	k+1
Incremental Implementation (Page 34)	1
Incremental Implementation (Page 34)	= Q + (r ‚àí Q )
Incremental Implementation (Page 34)	k k+1 k
Incremental Implementation (Page 34)	k+1
Incremental Implementation (Page 34)	NewEst = OldEst + StepSize(NewSample - OldEst)
Incremental Implementation (Page 34)	NewSample referred to as the ‚Äòtarget‚Äô value
Changing Reward Distributions (Page 36)	If the distributions remain the same, the
Changing Reward Distributions (Page 36)	problem is called stationary
Changing Reward Distributions (Page 36)	If the distributions are allowed to change
Changing Reward Distributions (Page 36)	over time, it is non-stationary
Changing Reward Distributions (Page 36)	Imagine having an average of 1 for a
Changing Reward Distributions (Page 36)	million time steps, and then changing to
Changing Reward Distributions (Page 36)	100. Updating the average would be slow
Non-stationary Problems (Page 37)	Averaging works fine for stationary rewards, but
Non-stationary Problems (Page 37)	not if it changes over time
Non-stationary Problems (Page 37)	Want to weight recent rewards more than old
Non-stationary Problems (Page 37)	ones, in case the values change
Non-stationary Problems (Page 37)	Use a constant step-size parameter 0 < Œ± ‚â§ 1
Non-stationary Problems (Page 37)	Q = Q + Œ± (r ‚Äì Q )
Non-stationary Problems (Page 37)	k+1 k k+1 k
Incremental Update Example (Page 38)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 38)	k+1 k k+1 k
Incremental Update Example (Page 38)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 38)	k+1
Incremental Update Example (Page 38)	Q = 50, Œ± = 1, r = 100
Incremental Update Example (Page 38)	k k+1
Incremental Update Example (Page 38)	Q = 50 + 1*(100-50) = 50+50 = 100
Incremental Update Example (Page 38)	k+1
Incremental Update Example (Page 38)	Q = 50, Œ± = 0.5, r = 100
Incremental Update Example (Page 38)	k k+1
Incremental Update Example (Page 38)	Q = 50 + 0.5*(100-50) = 75
Incremental Update Example (Page 38)	k+1
Incremental Update Example (Page 39)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 39)	k+1 k k+1 k
Incremental Update Example (Page 39)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 39)	k+1
Incremental Update Example (Page 39)	Q = 50, Œ± = 1, r = 100
Incremental Update Example (Page 39)	k k+1
Incremental Update Example (Page 39)	Q = 50 + 1*(100-50) = 50+50 = 100
Incremental Update Example (Page 39)	k+1
Incremental Update Example (Page 39)	Q
Incremental Update Example (Page 39)	k
Incremental Update Example (Page 39)	0 50 100
Incremental Update Example (Page 40)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 40)	k+1 k k+1 k
Incremental Update Example (Page 40)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 40)	k+1
Incremental Update Example (Page 40)	Q = 50, Œ± = 1, r = 100
Incremental Update Example (Page 40)	k k+1
Incremental Update Example (Page 40)	Q = 50 + 1*(100-50) = 50+50 = 100
Incremental Update Example (Page 40)	k+1
Incremental Update Example (Page 40)	r
Incremental Update Example (Page 40)	Q
Incremental Update Example (Page 40)	k+1
Incremental Update Example (Page 40)	k
Incremental Update Example (Page 40)	0 50 100
Incremental Update Example (Page 41)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 41)	k+1 k k+1 k
Incremental Update Example (Page 41)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 41)	k+1
Incremental Update Example (Page 41)	Q = 50, Œ± = 1, r = 100
Incremental Update Example (Page 41)	k k+1
Incremental Update Example (Page 41)	Q = 50 + 1*(100-50) = 50+50 = 100
Incremental Update Example (Page 41)	k+1
Incremental Update Example (Page 41)	Œ±
Incremental Update Example (Page 41)	r
Incremental Update Example (Page 41)	Q
Incremental Update Example (Page 41)	k+1
Incremental Update Example (Page 41)	k
Incremental Update Example (Page 41)	0 50 100
Incremental Update Example (Page 42)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 42)	k+1 k k+1 k
Incremental Update Example (Page 42)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 42)	k+1
Incremental Update Example (Page 42)	Q = 50, Œ± = 1, r = 100
Incremental Update Example (Page 42)	k k+1
Incremental Update Example (Page 42)	Q = 50 + 1*(100-50) = 50+50 = 100
Incremental Update Example (Page 42)	k+1
Incremental Update Example (Page 42)	Q
Incremental Update Example (Page 42)	k+1
Incremental Update Example (Page 42)	0 50 100
Incremental Update Example (Page 43)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 43)	k+1 k k+1 k
Incremental Update Example (Page 43)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 43)	k+1
Incremental Update Example (Page 43)	Q = 50, Œ± = 0.5, r = 100
Incremental Update Example (Page 43)	k k+1
Incremental Update Example (Page 43)	Q = 50 + 0.5*(100-50) = 75
Incremental Update Example (Page 43)	k+1
Incremental Update Example (Page 43)	Q
Incremental Update Example (Page 43)	k
Incremental Update Example (Page 43)	0 50 100
Incremental Update Example (Page 44)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 44)	k+1 k k+1 k
Incremental Update Example (Page 44)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 44)	k+1
Incremental Update Example (Page 44)	Q = 50, Œ± = 0.5, r = 100
Incremental Update Example (Page 44)	k k+1
Incremental Update Example (Page 44)	Q = 50 + 0.5*(100-50) = 75
Incremental Update Example (Page 44)	k+1
Incremental Update Example (Page 44)	r
Incremental Update Example (Page 44)	Q
Incremental Update Example (Page 44)	k+1
Incremental Update Example (Page 44)	k
Incremental Update Example (Page 44)	0 50 100
Incremental Update Example (Page 45)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 45)	k+1 k k+1 k
Incremental Update Example (Page 45)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 45)	k+1
Incremental Update Example (Page 45)	Q = 50, Œ± = 0.5, r = 100
Incremental Update Example (Page 45)	k k+1
Incremental Update Example (Page 45)	Q = 50 + 0.5*(100-50) = 75
Incremental Update Example (Page 45)	k+1
Incremental Update Example (Page 45)	Œ±
Incremental Update Example (Page 45)	r
Incremental Update Example (Page 45)	Q
Incremental Update Example (Page 45)	k+1
Incremental Update Example (Page 45)	k
Incremental Update Example (Page 45)	0 50 100
Incremental Update Example (Page 46)	Q = Q + Œ± (r ‚Äì Q )
Incremental Update Example (Page 46)	k+1 k k+1 k
Incremental Update Example (Page 46)	New estimate pulled toward r by Œ±
Incremental Update Example (Page 46)	k+1
Incremental Update Example (Page 46)	Q = 50, Œ± = 0.5, r = 100
Incremental Update Example (Page 46)	k k+1
Incremental Update Example (Page 46)	Q = 50 + 0.5*(100-50) = 75
Incremental Update Example (Page 46)	k+1
Incremental Update Example (Page 46)	Q
Incremental Update Example (Page 46)	k+1
Incremental Update Example (Page 46)	0 50 75 100
Greedy Action Selection (Page 59)	How to select an action from value estimations?
Greedy Action Selection (Page 59)	Simplest way: Greedy Selection
Greedy Action Selection (Page 59)	Select on play t, a greedy action a* for which:
Greedy Action Selection (Page 59)	Q (a*) = max Q (a)
Greedy Action Selection (Page 59)	t a t
Greedy Action Selection (Page 59)	This method always exploits current knowledge
Greedy Action Selection (Page 59)	to maximize immediate reward
Greedy Action Selection (Page 59)	No sampling or exploration to determine values
Greedy Action Selection (Page 59)	of another action to see if they may be better
Œµ-Greedy Selection (Page 61)	To add exploration, choose a random action with
Œµ-Greedy Selection (Page 61)	small probability Œµ
Œµ-Greedy Selection (Page 61)	In the limit, as the number of plays increases,
Œµ-Greedy Selection (Page 61)	each action will be sampled infinite times
Œµ-Greedy Selection (Page 61)	This guarantees k -> infinity, and Q (a)
Œµ-Greedy Selection (Page 61)	a t
Œµ-Greedy Selection (Page 61)	converges to Q*(a)
Œµ-Greedy Selection (Page 61)	In theory this works, but in practice it may take
Œµ-Greedy Selection (Page 61)	a very, very long time to converge
Bandit Algorithm Implementation (Page 65)	Choose value update method
Bandit Algorithm Implementation (Page 65)	1.
Bandit Algorithm Implementation (Page 65)	Choose action selection method
Bandit Algorithm Implementation (Page 65)	2.
Bandit Algorithm Implementation (Page 65)	Repeat
Bandit Algorithm Implementation (Page 65)	3.
Bandit Algorithm Implementation (Page 65)	Choose an action to perform
Bandit Algorithm Implementation (Page 65)	1.
Bandit Algorithm Implementation (Page 65)	Get the reward value from bandit
Bandit Algorithm Implementation (Page 65)	2.
Bandit Algorithm Implementation (Page 65)	Update your value for the action
Bandit Algorithm Implementation (Page 65)	3.
Bandit Algorithm Implementation (Page 66)	Incremental Avg + Œµ-Greedy
Bandit Algorithm Implementation (Page 66)	Function BanditAlgorithm(bandits)
Bandit Algorithm Implementation (Page 66)	Q[] = zeros(bandits.size)
Bandit Algorithm Implementation (Page 66)	1.
Bandit Algorithm Implementation (Page 66)	N[] = zeros(bandits.size)
Bandit Algorithm Implementation (Page 66)	2.
Bandit Algorithm Implementation (Page 66)	while (true)
Bandit Algorithm Implementation (Page 66)	3.
Bandit Algorithm Implementation (Page 66)	action = null
Bandit Algorithm Implementation (Page 66)	4.
Bandit Algorithm Implementation (Page 66)	if (rand() < Œµ) action = randomAction()
Bandit Algorithm Implementation (Page 66)	5.
Bandit Algorithm Implementation (Page 66)	else action = argmax (Q[a])
Bandit Algorithm Implementation (Page 66)	6.
Bandit Algorithm Implementation (Page 66)	a
Bandit Algorithm Implementation (Page 66)	R = bandits[action].getValue()
Bandit Algorithm Implementation (Page 66)	7.
Bandit Algorithm Implementation (Page 66)	N[action] = N[action] + 1
Bandit Algorithm Implementation (Page 66)	8.
Bandit Algorithm Implementation (Page 66)	Q[a] = Q[a] + (1.0 / N[a])*(R ‚Äì Q[a])
Bandit Algorithm Implementation (Page 66)	9.
Upper Confidence Bound (UCB) (Page 67)	Exploration needed because there is uncertainty
Upper Confidence Bound (UCB) (Page 67)	about the accuracy of action value estimates
Upper Confidence Bound (UCB) (Page 67)	Greedy looks good now, maybe not the best
Upper Confidence Bound (UCB) (Page 67)	Œµ-Greedy explores, but randomly
Upper Confidence Bound (UCB) (Page 67)	Would be better to select among non-greedy
Upper Confidence Bound (UCB) (Page 67)	actions that may be close to optimal, with some
Upper Confidence Bound (UCB) (Page 67)	measure for how certain we are about their values
Upper Confidence Bound (UCB) (Page 67)	The UCB selection method does this
UCB (Page 68)	ln(t) = natural log
UCB (Page 68)	N (a) = number of times a was selected
UCB (Page 68)	t
UCB (Page 68)	c > 0 controls the degree of exploration
UCB (Page 68)	Typically, all values tried once first
UCB (Page 68)	sqrt term is the uncertainty of value estimate for action a
UCB (Page 68)	Function calculates a sort of ‚Äòupper bound‚Äô on the possible
UCB (Page 68)	true value of action a
UCB (Page 68)	As N (a) increases, the uncertainty goes down
UCB (Page 68)	t
UCB (Page 68)	When t increases but not Nt(a), uncertainty goes up
Q(a) vs Q(s,a) (Page 70)	Q(a) = value of doing action a
Q(a) vs Q(s,a) (Page 70)	The value of a specific action will vary
Q(a) vs Q(s,a) (Page 70)	depending on the state it was issued
Q(a) vs Q(s,a) (Page 70)	For example: Moving up is good if the
Q(a) vs Q(s,a) (Page 70)	goal is up, but not if the goal is down
Q(a) vs Q(s,a) (Page 70)	Q(s,a) = value of action a at state s
Exam Questions (Page 75)	Formula for Average, Incremental update
Exam Questions (Page 75)	Be able to do an example like in slides
Exam Questions (Page 75)	Exploitation vs Exploration
Exam Questions (Page 75)	Action Selection Methods
Exam Questions (Page 75)	Greedy, Epsilon Greedy, UCB
Exam Questions (Page 75)	Effect of Epsilon as it goes up or down
Exam Questions (Page 75)	Bandit Algorithm
